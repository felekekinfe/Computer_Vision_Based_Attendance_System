{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test video feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2.485] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@2.520] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow('video feed',frame)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2.801] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@2.802] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "face_detector=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_detector.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv2.imshow('video feed',frame)\n",
    "\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop Face to be appropriate for facenet model 160X160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/cs/Desktop/Project/Computer_Vision_Based_Attendance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'dataset/main.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m person_input\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir,person)\n\u001b[1;32m     13\u001b[0m person_output\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir,person)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperson_input\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     18\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(person_input,img_file)\n\u001b[1;32m     19\u001b[0m     img\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'dataset/main.py'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "face_detector=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "input_dir='dataset'\n",
    "output_dir='cropped_dataset'\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for person in os.listdir(input_dir):\n",
    "    person_input=os.path.join(input_dir,person)\n",
    "    person_output=os.path.join(output_dir,person)\n",
    "\n",
    "    \n",
    "    \n",
    "    for img_file in os.listdir(person_input):\n",
    "        img_path=os.path.join(person_input,img_file)\n",
    "        img=cv2.imread(img_path)\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        face=face_detector.detectMultiScale(gray,1.1,5)\n",
    "        if len(face)>0:\n",
    "            x,y,w,h = face[0]\n",
    "            face=img[y:y+h,x:x+w]\n",
    "            face=cv2.resize(face,(160,160))\n",
    "            output_path=os.path.join(person_output,img_file)\n",
    "            cv2.imwrite(output_path,face)\n",
    "print('face cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face cropping process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "input_dir = 'dataset'\n",
    "output_dir = 'cropped_dataset'\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "for person in os.listdir(input_dir):\n",
    "    person_input = os.path.join(input_dir, person)\n",
    "    \n",
    "    # Skip if it's not a directory (e.g., it's a file like main.py)\n",
    "    if not os.path.isdir(person_input):\n",
    "        continue\n",
    "\n",
    "    person_output = os.path.join(output_dir, person)\n",
    "\n",
    "    # Create the person's output directory if it doesn't exist\n",
    "    if not os.path.exists(person_output):\n",
    "        os.makedirs(person_output)\n",
    "\n",
    "    # Loop through each image file for that person\n",
    "    for img_file in os.listdir(person_input):\n",
    "        img_path = os.path.join(person_input, img_file)\n",
    "\n",
    "        # Check if the current item is a file\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "       \n",
    "        if img is None:\n",
    "            print(f\"Skipping {img_file} as it cannot be read.\")\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image\n",
    "        faces = face_detector.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "        # If faces are detected, crop and save them\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]\n",
    "            face = img[y:y + h, x:x + w]\n",
    "            face = cv2.resize(face, (160, 160))  # Resize the face to 160x160\n",
    "            output_path = os.path.join(person_output, img_file)\n",
    "            cv2.imwrite(output_path, face)\n",
    "        else:\n",
    "            print(f\"No face detected in {img_file}, skipping.\")\n",
    "\n",
    "print('Face cropping process completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     z\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(z)\n",
      "\u001b[0;31mTypeError\u001b[0m: join() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "x={'a':'b'}\n",
    "\n",
    "for i,j in x.items():\n",
    "    z=''.join(i,j)\n",
    "    print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
